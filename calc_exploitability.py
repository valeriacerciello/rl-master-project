# calc_exploitability.py

from __future__ import annotations

import numpy as np
import numpy.typing as npt
import os

__all__ = [
    "calc_exploitability_true",
    "policy_value_zero_sum",
    "value_iteration",
    "plot_results",
]

# ===============
# Helpers
# ===============

def _as_array(x: npt.ArrayLike, dtype=None) -> np.ndarray:
    return np.asarray(x, dtype=dtype)


def _check_gamma(gamma: float) -> None:
    if not (0.0 <= gamma < 1.0):
        raise ValueError(f"gamma must be in [0, 1), got {gamma}.")


def _check_shapes(
    mu_pi: np.ndarray,
    nu_pi: np.ndarray,
    reward: np.ndarray,
    transition: np.ndarray,
    initial_dist: np.ndarray,
) -> tuple[int, int, int]:
    """
    Validates and returns inferred (S, A1, A2).
    Expected shapes:
      mu_pi        : (S, A1)
      nu_pi        : (S, A2)
      reward       : (S, A1, A2)
      transition   : (S, A1, A2, S)
      initial_dist : (S,)
    """
    if mu_pi.ndim != 2 or nu_pi.ndim != 2:
        raise ValueError("mu_pi and nu_pi must be 2D (S, A).")
    if reward.ndim != 3:
        raise ValueError("reward must be 3D (S, A1, A2).")
    if transition.ndim != 4:
        raise ValueError("transition must be 4D (S, A1, A2, S').")
    if initial_dist.ndim != 1:
        raise ValueError("initial_dist must be 1D (S,).")

    S_mu, A1 = mu_pi.shape
    S_nu, A2 = nu_pi.shape
    S_r, A1_r, A2_r = reward.shape
    S_p, A1_p, A2_p, S_p_next = transition.shape
    S_id = initial_dist.shape[0]

    if not (S_mu == S_nu == S_r == S_p == S_p_next == S_id):
        raise ValueError(
            f"Inconsistent state dims: "
            f"mu_pi={S_mu}, nu_pi={S_nu}, reward={S_r}, transition={S_p}/{S_p_next}, initial={S_id}"
        )
    if not (A1 == A1_r == A1_p and A2 == A2_r == A2_p):
        raise ValueError(
            f"Inconsistent action dims: "
            f"A1={A1} vs reward={A1_r} vs transition={A1_p}; "
            f"A2={A2} vs reward={A2_r} vs transition={A2_p}"
        )

    # Simple probabilistic sanity checks (row-stochastic per state)
    if not np.allclose(mu_pi.sum(axis=1), 1.0, atol=1e-6):
        raise ValueError("Rows of mu_pi must sum to 1.")
    if not np.allclose(nu_pi.sum(axis=1), 1.0, atol=1e-6):
        raise ValueError("Rows of nu_pi must sum to 1.")
    if not np.allclose(initial_dist.sum(), 1.0, atol=1e-6):
        raise ValueError("initial_dist must sum to 1.")

    # Transition should be row-stochastic under (s,a,b)
    row_sums = transition.reshape(S_p * A1_p * A2_p, S_p_next).sum(axis=1)
    if not np.allclose(row_sums, 1.0, atol=1e-6):
        raise ValueError("transition rows (per s,a,b) must sum to 1.")

    return S_mu, A1, A2


# ===============
# Exploitability
# ===============

def calc_exploitability_true(
    mu_pi: npt.NDArray[np.floating],
    nu_pi: npt.NDArray[np.floating],
    reward: npt.NDArray[np.floating],
    transition: npt.NDArray[np.floating],
    initial_dist: npt.NDArray[np.floating],
    gamma: float,
) -> float:
    """
    Exact exploitability of a joint policy (μ, ν) in a zero-sum Markov game.

    Exploitability := max( BR(μ) - V(μ,ν), BR(ν) - V(μ,ν) ),
    where BR(μ) is the best-response value of μ against ν (maximizer),
    and BR(ν) is the best-response value of ν against μ (minimizer),
    with ν’s reward negated accordingly.

    Args:
        mu_pi        : (S, A1) policy of the maximizing player μ
        nu_pi        : (S, A2) policy of the minimizing player ν
        reward       : (S, A1, A2) scalar payoff to μ (ν receives -reward)
        transition   : (S, A1, A2, S) state transition probabilities
        initial_dist : (S,) start-state distribution
        gamma        : discount factor in [0, 1)

    Returns:
        float: exploitability (0 at Nash).
    """
    mu_pi = _as_array(mu_pi, dtype=float)
    nu_pi = _as_array(nu_pi, dtype=float)
    reward = _as_array(reward, dtype=float)
    transition = _as_array(transition, dtype=float)
    initial_dist = _as_array(initial_dist, dtype=float)

    _check_gamma(gamma)
    _, A1, A2 = _check_shapes(mu_pi, nu_pi, reward, transition, initial_dist)

    # Joint value under (μ, ν)
    V_joint = policy_value_zero_sum(
        mu_pi=mu_pi,
        nu_pi=nu_pi,
        transition=transition,
        reward=reward,
        initial_dist=initial_dist,
        gamma=gamma,
    )

    # μ's induced single-agent MDP against fixed ν
    # R_mu[s,a] = E_b[ r(s,a,b) ]
    R_mu = (reward * nu_pi[:, None, :]).sum(axis=2)  # (S, A1)
    # P_mu[s,a,s'] = sum_b ν(b|s) P[s,a,b,s']
    P_mu = (transition * nu_pi[:, None, :, None]).sum(axis=2)  # (S, A1, S)
    V_br_mu = value_iteration(R_mu, P_mu, initial_dist=initial_dist, gamma=gamma)

    # ν's induced single-agent MDP (minimizer → negate reward)
    # R_nu[s,b] = - E_a[ r(s,a,b) ]
    R_nu = -(reward * mu_pi[:, :, None]).sum(axis=1)  # (S, A2)
    # P_nu[s,b,s'] = sum_a μ(a|s) P[s,a,b,s']
    P_nu = (transition * mu_pi[:, :, None, None]).sum(axis=1)  # (S, A2, S)
    V_br_nu = value_iteration(R_nu, P_nu, initial_dist=initial_dist, gamma=gamma)

    return float(max(V_br_mu - V_joint, V_br_nu - V_joint))


# =======================
# Evaluation of V^{μ,ν}
# =======================

def policy_value_zero_sum(
    mu_pi: npt.NDArray[np.floating],
    nu_pi: npt.NDArray[np.floating],
    transition: npt.NDArray[np.floating],
    reward: npt.NDArray[np.floating],
    initial_dist: npt.NDArray[np.floating],
    gamma: float,
) -> float:
    """
    V^{μ,ν} = E[ ∑_{t≥0} γ^t r(s_t, a_t, b_t) ] under joint policy (μ, ν).

    Solves (I - γ P_π) V = r_π, then returns initial_dist · V.
    """
    _check_gamma(gamma)

    # r_π(s) = E_{a∼μ, b∼ν} [r(s,a,b)]
    r_s = (reward * mu_pi[:, :, None] * nu_pi[:, None, :]).sum(axis=(1, 2))  # (S,)

    # P_π[s,s'] = E_{a,b}[ P[s,a,b,s'] ]
    P_joint = (transition * mu_pi[:, :, None, None] * nu_pi[:, None, :, None]).sum(axis=(1, 2))  # (S,S)

    I = np.eye(P_joint.shape[0], dtype=float)
    A = I - gamma * P_joint

    # Solve (I - γP)V = r
    try:
        V = np.linalg.solve(A, r_s)
    except np.linalg.LinAlgError:
        # Fallback to least squares if A is near-singular
        V = np.linalg.lstsq(A, r_s, rcond=None)[0]

    return float(initial_dist @ V)


# ============================
# Single-agent Value Iteration
# ============================

def value_iteration(
    R: npt.NDArray[np.floating],
    P: npt.NDArray[np.floating],
    initial_dist: npt.NDArray[np.floating],
    tol: float = 1e-6,
    max_iter: int = 10_000,
    gamma: float = 0.9,
) -> float:
    """
    Standard value iteration.

    Args:
        R            : (S, A) immediate rewards
        P            : (S, A, S) transition model
        initial_dist : (S,) start-state distribution (used only for returning a scalar)
        tol          : convergence threshold on ||V_{k+1} - V_k||_∞
        max_iter     : safety cap on iterations
        gamma        : discount factor in [0, 1)

    Returns:
        float: initial_dist · V*, where V* is the optimal state-value.
    """
    R = _as_array(R, dtype=float)
    P = _as_array(P, dtype=float)
    initial_dist = _as_array(initial_dist, dtype=float)

    _check_gamma(gamma)

    if R.ndim != 2 or P.ndim != 3:
        raise ValueError("Shapes must be R:(S,A), P:(S,A,S).")
    S, A = R.shape
    if P.shape != (S, A, S):
        raise ValueError(f"P must have shape (S,A,S); got {P.shape}.")
    if initial_dist.shape != (S,):
        raise ValueError(f"initial_dist must have shape (S,); got {initial_dist.shape}.")
    if not np.allclose(initial_dist.sum(), 1.0, atol=1e-6):
        raise ValueError("initial_dist must sum to 1.")

    V = np.zeros(S, dtype=float)
    for _ in range(max_iter):
        # Q(s,a) = R[s,a] + γ · Σ_{s'} P[s,a,s'] V[s']
        Q = R + gamma * (P @ V)
        V_new = Q.max(axis=1)
        if np.max(np.abs(V_new - V)) < tol:
            V = V_new
            break
        V = V_new
    return float(initial_dist @ V)


# ===========
# Plotting
# ===========

def plot_results(
    all_queries_magail: list[npt.ArrayLike],
    all_exploits_magail: list[npt.ArrayLike],  
    all_queries_bc: list[npt.ArrayLike],
    all_exploits_bc: list[npt.ArrayLike],
    name: str | None = None,  # kept for API compatibility (unused)
) -> None:
    """
    Plot exploitability vs. queries for MAGAIL and/or BC.

    Each method expects lists of runs. For each run, queries and exploitability
    arrays should be aligned. Runs are trimmed to the minimum common length
    within each method before aggregation.
    """
    import numpy as np
    import matplotlib.pyplot as plt

    def _stack_mean_std(xs_list, ys_list):
        if len(xs_list) == 0:
            return None
        xs_arr = [np.asarray(x, dtype=float) for x in xs_list]
        ys_arr = [np.asarray(y, dtype=float) for y in ys_list]
        L = min(min(len(x) for x in xs_arr), min(len(y) for y in ys_arr))
        xs = np.vstack([x[:L] for x in xs_arr])
        ys = np.vstack([y[:L] for y in ys_arr])
        return xs.mean(axis=0), ys.mean(axis=0), ys.std(axis=0)

    has_magail = len(all_queries_magail) > 0 and len(all_exploits_magail) > 0
    has_bc = len(all_queries_bc) > 0 and len(all_exploits_bc) > 0

    agg_mag = _stack_mean_std(all_queries_magail, all_exploits_magail) if has_magail else None
    agg_bc = _stack_mean_std(all_queries_bc, all_exploits_bc) if has_bc else None

    plt.figure(figsize=(9, 5.5))
    title = "BC vs. MAGAIL" if (has_magail and has_bc) else ("MAGAIL" if has_magail else "BC")

    if agg_mag is not None:
        q_m, e_m, s_m = agg_mag
        plt.plot(q_m, e_m, label="MAGAIL")
        plt.fill_between(q_m, e_m - s_m, e_m + s_m, alpha=0.18)

    if agg_bc is not None:
        q_b, e_b, s_b = agg_bc
        plt.plot(q_b, e_b, label="BC")
        plt.fill_between(q_b, e_b - s_b, e_b + s_b, alpha=0.18)

    plt.xlabel("Queries / Iterations")
    plt.ylabel("Exploitability")
    plt.title(title)
    plt.legend()
    plt.grid(True, linewidth=0.5, alpha=0.6)
    plt.tight_layout()

    out_path = os.path.join("plots", "exploitability_exp", "Exploitability_BC_vs_MAGAIL.png")
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.show()